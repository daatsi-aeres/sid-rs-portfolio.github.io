<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-03T19:45:48-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sai Aneesh Suryadevara</title><entry><title type="html">Introduction to Tensors</title><link href="http://localhost:4000/blog/test" rel="alternate" type="text/html" title="Introduction to Tensors" /><published>2024-03-11T00:00:00-04:00</published><updated>2024-03-11T00:00:00-04:00</updated><id>http://localhost:4000/blog/test</id><content type="html" xml:base="http://localhost:4000/blog/test"><![CDATA[<script type="text/javascript" id="MathJax-script" defer
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

The zeros of a function can give us a lot of information regarding the function. Why specifically zero? Division is in general more difficult to implement in comparison to multiplication. Division is implemented via the solution to roots of an equation in computers. There are other reasons such as eigenvalue decomposition. We shall see all of this later in the course. 

***Theorem.*** *Intermediate Value Theorem*. If $$f: [a, b] \to \mathbb R$$ is a continuous function with $$f(a)\cdot f(b) < 0$$ then there is a $$c \in [a, b]$$ such that $$f(x) = 0$$.

### Bisection method

The bisection method is implemented using the above theorem. To begin, we set $$a_1 = a$$ and $$b_1 = b$$, and let $$p_1$$ be the midpoint of $$[a, b]$$; 

- If $$f(p_1) = 0$$, then we are done
- If $$f(p_1)f(a_1)>0$$, then the subinterval $$[p_1, b_1]$$ contains a root of $$f$$, we then set $$a_2 = p_1$$ and $$b_2 = b_1$$
- If $$f(p_1)f(b_1)>0$$, then the subinterval $$[a_1, p_1]$$ contains a root of $$f$$, we then set $$a_2 = a_1$$ and $$b_2 = p_1$$

We continue this process until we obtain the root. Each iteration reduces the size of the interval by half. Therefore, it is beneficial to choose a small interval in the beginning. The sequence of $$\{p_i\}$$ is a Cauchy sequence. IT may happen that none of the $$p_i$$ is a root. For instance, the root may be an irrational number and $$a_1, b_1$$ may be rational numbers. There are three criteria to decide to stop the bisection process.

- $$\|f(p_n)\| < \epsilon$$ - not very good because it may take a long time to achieve it, or that it may be achieved easily and yet $$p_n$$ is significantly many steps away from a root of $$f$$ 
- $$\|p_n - p_{n-1}\| <\epsilon$$ is not good because it does not take the function $$f$$ into account. While $$p_n$$ and $$p_{n-1}$$ may be close enough but the actual root may still be many steps away.
- $$p_n \neq 0 \text{ and } \frac{\|p_n - p_{n-1}\|}{\|p_n\|} < \epsilon$$ looks enticing as it mimics the relative error but again it does not take $$f$$ into account.]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[An introduction to Tensors and notation. Summary of a few papers.]]></summary></entry><entry><title type="html">Introduction to Reinforcement Learning</title><link href="http://localhost:4000/blog/rl" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning" /><published>2024-02-05T00:00:00-05:00</published><updated>2024-02-05T00:00:00-05:00</updated><id>http://localhost:4000/blog/rl</id><content type="html" xml:base="http://localhost:4000/blog/rl"><![CDATA[<script type="text/javascript" id="MathJax-script" defer
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

Reinforcement Learning (RL) is essentially learning from experiences. The basic premise of Reinforcement Learning is that we have an **Agent**, that performs actions in an **Environment**, and based on the feedback it receives, it learns over time, the correct action to take at every state. 

<div class="side-note">
RL seems very intuitive, right? Isn't this how we, as children learnt things? This direction of thinking may lead us to believe that RL might be the way to achieve AGI. However, with <strong>General Intelligence</strong>, the agent should be able to keep learning new tasks, without forgetting the skills it has learnt and also use it's existing knowledge as prior to learn new tasks. But the framework of RL leads to the agent making the best decisions on the current environment, and we get an agent specialised at a particular task. 
</div>

Let us try to formulate the problem mathematically. 

![MDP]({{site.url}}/images/rl_intro/mdp.png "MDP")


> **Assumption 1: Complete Observability**  
The agent gets to observe the complete state. Fully observable enviroment means that \\
 Observation ($$O_t$$) = Agent State ($$S_t^a$$) = Environment State ($$S_t^e$$) = State ($$S_t$$).

 **History**  
  Defined as the sequence of obervations, actions, rewards, $$H_t =  A_1, O_1, R_1,...., A_t, O_t, R_t$$

<div class="side-note">
<strong> State </strong>: History determines what happpens next. But it is not a helpful representation as it can become too large. State is essentially a concise summary that captures all the information we need to determine what happens next. For example, State can be the last 4 observations.
</div>

 **State Function**  
  The state at any time is a function of its history: $$S_t = f(H_t)$$



> **Assumption 2: Markov Property**  
The next state $$S'$$ depends solely on the current state $$S$$. This is called the **Markov Property**. In simple terms it means "future outcomes depends solely on the present state- it doesn't matter how we got to this state." \\
\begin{equation}
  \mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, S_2, ..., S_t]
\end{equation}

 - If considering the State as the Environment State $$S_t^e$$, it is inherently Markov.
 - Viewing the State as the History $$H_t$$ also conforms to Markov properties.

 When we work with these two assumptions, we call it a **Markov Decision Process**

> A **Markov Decision Process** is a tuple $$ \langle \mathcal{S, A, P, R}, \gamma \rangle $$
- $$ \mathcal{S} $$ is a finite set of states
- $$ \mathcal{A} $$ is a finite set of actions
- $$ \mathcal{P} $$ is a state transition probability matrix,  
\begin{equation}
 \mathcal{P}(s'|s, a) = \mathbb{P}[S_{t+1} = s' \mid S_t = s, A_t = a] 
\end{equation}
- $$ \mathcal{R} $$ is a reward function,  $$R$$ is the reward
\begin{equation} 
\mathcal{R}(s,a) = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a] 
\end{equation}
- $$ \gamma $$ is a discount factor $$ \gamma \in [0, 1] $$


There also exist partially observable markov decision processes (**POMDPs**) where the complete state of the system is not observable / the agent indirectly observes the environment. An example: The game of chess is an MDP, the state is completely observable, however, the a game of poker is a POMDP, where we do not know which cards the opponents have. In POMDPs, we don't know the environment state. ( Agent State $$\neq$$ Environment State). So in this case, the agent must construct its own representation of the state $$S_t^a$$ e.g.,
  - Complete history: $$S_t^a = H_t$$
  - Beliefs of environment state: $$S_t^a = \left( P[S_t^e = s^1], \ldots, P[S_t^e = s^n] \right)$$

<h2 class="post-title">Policy, Value-Functions and The Bellman Equations</h2>

The **return** $$ G_t $$ is the total discounted reward from time-step $$ t $$.

$$ G_t = R_{t+1} + \gamma R_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

- The value of receiving reward $$ R $$ after $$ k + 1 $$ time-steps is $$ \gamma^k R $$.
- $$ \gamma \to 0$$, implies we care about immediate reward and  $$\gamma \to 1$$, implies we care are far-sighted

A **policy** $$ \pi $$ is a distribution over actions given states,

$$ \pi(a|s) = \mathbb{P}[A_t = a | S_t = s] $$

- A policy fully defines the behaviour of an agent
- MDP policies depend on the current state (not the history)
- i.e., Policies are **stationary** (time-independent), $$ A_t \sim \pi( . \mid S_t), \forall t > 0 $$

The **state-value function** $$ V_{\pi}(s) $$ for an MDP is the expected return starting from state  $$s  $$, and then following policy  $$\pi $$:

$$ V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid S_t = s] $$

<div class="side-note">
<strong> Value of a State </strong>: From this state, if I take the best possible action at each subsequent step, what is the long-term reward I can expect?
</div>

The **action-value function**  $$ Q_{\pi}(s, a)  $$ is the expected return starting from state  $$ s  $$, taking action  $$ a  $$, and then following policy  $$ \pi  $$:

$$ Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \mid S_t = s, A_t = a] $$


The state-value function can again be decomposed into immediate reward plus discounted value of successor state, 

$$ V_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] $$ 


Since, $$ \mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X \mid Y]] $$ ( from Law of total expectation)

$$ V_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma \mathbb{E}_{\pi}[G_{t+1} \mid S_{t+1} = s'] \mid S_t = s] $$ 

$$ V_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) \mid S_t = s] $$

$$ V_{\pi}(s) =  \mathbb{E}_{\pi}[R_{t+1} \mid S_t = s] + \gamma \mathbb{E}_{\pi}[V_{\pi}(S_{t+1}) \mid S_t = s] $$

$$ V_{\pi}(s) =  \sum_{r' \in R} r' \mathbb{P}(R_{t+1} = r' \mid S_t = s) + \gamma \sum_{s' \in \mathcal{S}}V_{\pi}(S_{t+1} = s') \mathbb{P}(S_{t+1} = s' | S_t = s) $$

$$ V_{\pi}(s) =   \sum_{r' \in R} r' \sum_{a \in \mathcal{A}} \mathbb{P}(R_{t+1} = r', A_t= a | S_t=s) + \gamma \sum_{s' \in \mathcal{S}}V_{\pi}(S_{t+1} = s') \sum_{a \in \mathcal{A}}\mathbb{P}(S_{t+1} = s', A_t = a | S_t = s)$$

$$ V_{\pi}(s) = \sum_{a \in \mathcal{A}}  \sum_{r' \in R} r' \mathbb{P}(R_{t+1} = r' | S_t=s, A_t= a)\mathbb{P}[A_t = a | S_t = s] + \gamma \sum_{s' \in \mathcal{S}}V_{\pi}(S_{t+1} = s') \sum_{a \in \mathcal{A}}\mathbb{P}(S_{t+1} = s', A_t = a | S_t = s)$$

$$ V_{\pi}(s) = \sum_{a \in \mathcal{A}} \mathbb{E}_{\pi}[R_{t+1} \mid S_t = s, A_t = a]\mathbb{P}[A_t = a | S_t = s] + \gamma \sum_{a \in \mathcal{A}}\mathbb{P}[A_t = a | S_t = s]\sum_{s' \in \mathcal{S}}\mathbb{P}(S_{t+1} = s' | S_t = s, A_t = a)V_{\pi}(S_{t+1} = s')$$

$$ V_{\pi}(s) = \sum_{a \in \mathcal{A}} \mathcal{R}(s,a)\pi(a|s) + \gamma \sum_{a \in \mathcal{A}}\pi(a|s) \sum_{s' \in \mathcal{S}}\mathcal{P}(s' | s,a)V_{\pi}(s') $$

The action-value function can similarly be decomposed,

$$ Q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma Q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a] $$

> **Bellman Equations**
\begin{equation}
V_{\pi}(s) = \sum_{a \in \mathcal{A}}\pi(a|s) \overset{Q_{\pi}(s,a)}{\left(\mathcal{R}(s,a) + \gamma \sum_{s' \in \mathcal{S}}\mathcal{P}(s' | s,a )V_{\pi}(s')\right)}
 \end{equation}
 \begin{equation}
 Q_{\pi}(s,a) = \mathcal{R}(s,a) + \gamma\sum_{s' \in S}\mathcal{P}(s' |s,a) \sum_{a' \in \mathcal{A}}\pi(a'|s') Q_{\pi}(s',a')
 \end{equation}

The **optimal state-value function** $$ V^* (s) $$ is the maximum value function over all policies:

$$
V^*(s) = \max_{\pi} V_{\pi}(s)
$$

The **optimal action-value function** $$ Q^*(s, a) $$ is the maximum action-value function over all policies:

$$
Q^*(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

The **Optimal Policy** is given by:
$$ \pi^*(a|s) = 
\begin{cases} 
1 & \text{if } a = \text{argmax}_{a \in A} Q^*(s, a) \\
0 & \text{otherwise} 
\end{cases} $$

We can write,

$$ V^*(s) = \max_a Q^*(s, a) $$

$$ Q^*(s,a) = \mathcal{R}(s,a) + \gamma\sum_{s' \in S}\mathcal{P}(s' \mid s,a) V^*(s') $$

> The **Bellman Optimality Equations** are given by:
>
>$$ V^*(s) = \max_a \left( \mathcal{R}(s,a) + \gamma\sum_{s' \in S}\mathcal{P}(s' \mid s,a) V^*(s') \right) $$
>
>$$ Q^* (s, a) = \mathcal{R}(s,a) + \gamma \sum_{s' \in S}\mathcal{P}(s'\mid s,a) \max_{a'} Q^* (s', a')  $$


<h2 class="post-title">Value Iteration and Policy Iteration</h2>

## Algorithm 1: Value Iteration

- **Inputs:** Reward Function $$\mathcal{R}(s,a)$$, state transition probabilties $$\mathcal{P}(s' \mid s,a)$$ 
- **Initialise** $$V^{(0)}(s) = 0 \:\:\:\:\:\forall s \in S$$
- While not converged, do:

    - For $$s \in S$$

    $$ V^{(t+1)}(s) \leftarrow \max_{a \in \mathcal{A}} \left( \mathcal{R}(s,a) +  \gamma\sum_{s' \in S} \mathcal{P}(s' \mid s,a) V^{(t)}(s') \right) $$

    $$ t = t+1 $$
- **Stopping:** whenever $$V^{(t+1)}(s) = V^{(t)}(s)$$, for all $$s$$, we must have found $$V^*$$

- **Policy:** 

$$\pi^*(s) \leftarrow argmax_{a \in \mathcal{A}} \left( \mathcal{R}(s,a) +  \gamma\sum_{s' \in S} \mathcal{P}(s' \mid s,a) V^{(t)}(s') \right) $$


## Algorithm 2: Policy Iteration

- **Inputs:** Reward Function $$\mathcal{R}(s,a)$$, state transition probabilties $$\mathcal{P}(s' \mid s,a)$$ 
- **Initialise** $$\pi$$ randomly
- While not converged, do:

    - Solve the Bellman Equations defined by policy $$\pi$$

    $$ V^{\pi}(s) = \mathcal{R}(s,\pi(s)) +  \gamma\sum_{s' \in S} \mathcal{P}(s' \mid s, \pi(s)) V^{\pi}(s') $$

    - update $$\pi$$

    $$ \pi(s) \leftarrow argmax_{a \in \mathcal{A}} \left( \mathcal{R}(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}(s' \mid s,a)V^{\pi}(s') \right) $$

- Return $$\pi$$]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[An introduction to RL concepts.]]></summary></entry><entry><title type="html">Learning Nonprehensile Dynamic Manipulation&amp;amp;#58 Sim2real Vision-based Policy with a Surgical Robot</title><link href="http://localhost:4000/blog/mitacs" rel="alternate" type="text/html" title="Learning Nonprehensile Dynamic Manipulation&amp;amp;#58 Sim2real Vision-based Policy with a Surgical Robot" /><published>2022-09-27T00:00:00-04:00</published><updated>2022-09-27T00:00:00-04:00</updated><id>http://localhost:4000/blog/mitacs</id><content type="html" xml:base="http://localhost:4000/blog/mitacs"><![CDATA[]]></content><author><name>Radian Gondokaryono, Mustafa Haiderbhai, &lt;b&gt;Sai Aneesh Suryadevara&lt;/b&gt;, Lueder Alexander Kahrs</name></author><category term="projects" /><category term="publications" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/_pages/research/mitacs/demo.jpg" /><media:content medium="image" url="http://localhost:4000/_pages/research/mitacs/demo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bachelor’s Thesis&amp;amp;#58 Control of Continuum Robots Using Deep Reinforcement Learning</title><link href="http://localhost:4000/blog/btp" rel="alternate" type="text/html" title="Bachelor’s Thesis&amp;amp;#58 Control of Continuum Robots Using Deep Reinforcement Learning" /><published>2020-12-01T00:00:00-05:00</published><updated>2020-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/btp</id><content type="html" xml:base="http://localhost:4000/blog/btp"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="research" /></entry><entry><title type="html">Flipkart National Competition&amp;amp;#58 Autonomous package delivery bots</title><link href="http://localhost:4000/blog/flipkart" rel="alternate" type="text/html" title="Flipkart National Competition&amp;amp;#58 Autonomous package delivery bots" /><published>2020-05-20T00:00:00-04:00</published><updated>2020-05-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/flipkart</id><content type="html" xml:base="http://localhost:4000/blog/flipkart"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="course" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/_pages/projects/flipkart_grid3/gridborder.png" /><media:content medium="image" url="http://localhost:4000/_pages/projects/flipkart_grid3/gridborder.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Decentralized Multi-Agent Patrolling using Q-Learning</title><link href="http://localhost:4000/blog/mrpp" rel="alternate" type="text/html" title="Decentralized Multi-Agent Patrolling using Q-Learning" /><published>2019-12-01T00:00:00-05:00</published><updated>2019-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/mrpp</id><content type="html" xml:base="http://localhost:4000/blog/mrpp"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="research" /></entry><entry><title type="html">e -Yantra Robotics Competition&amp;amp;#58 Autonomous Delivery Drone System</title><link href="http://localhost:4000/blog/eyantra" rel="alternate" type="text/html" title="e -Yantra Robotics Competition&amp;amp;#58 Autonomous Delivery Drone System" /><published>2019-12-01T00:00:00-05:00</published><updated>2019-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/eyantra</id><content type="html" xml:base="http://localhost:4000/blog/eyantra"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="course" /></entry><entry><title type="html">Image-to-Image Translation using CycleGAN and DiscoGAN</title><link href="http://localhost:4000/blog/gnr638" rel="alternate" type="text/html" title="Image-to-Image Translation using CycleGAN and DiscoGAN" /><published>2018-12-01T00:00:00-05:00</published><updated>2018-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/gnr638</id><content type="html" xml:base="http://localhost:4000/blog/gnr638"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="course" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/_pages/projects/gnr638/gan.png" /><media:content medium="image" url="http://localhost:4000/_pages/projects/gnr638/gan.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Statistical Solvers using Graph Neural Networks</title><link href="http://localhost:4000/blog/ie643" rel="alternate" type="text/html" title="Statistical Solvers using Graph Neural Networks" /><published>2017-06-12T00:00:00-04:00</published><updated>2017-06-12T00:00:00-04:00</updated><id>http://localhost:4000/blog/ie643</id><content type="html" xml:base="http://localhost:4000/blog/ie643"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="course" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/_pages/projects/ie643/ie643.png" /><media:content medium="image" url="http://localhost:4000/_pages/projects/ie643/ie643.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Self Driving Car, University of Toronto</title><link href="http://localhost:4000/blog/selfdriving" rel="alternate" type="text/html" title="Self Driving Car, University of Toronto" /><published>2016-12-01T00:00:00-05:00</published><updated>2016-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/selfdriving</id><content type="html" xml:base="http://localhost:4000/blog/selfdriving"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="course" /></entry></feed>